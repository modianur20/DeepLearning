{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward Network",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "povQmBoMSPV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Step 1: Loading MNIST Train Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "## Step 2: Make Dataset Iterable\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 6000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS21yCNOSYer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yimG3eIU8QU",
        "colab_type": "text"
      },
      "source": [
        "**MODEL_1_onehiddenlayer_Sigmoid**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbA-CzAUSY1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "1f2b4e5e-1571-482c-f69d-2e6ab1356af1"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim = 64\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 600 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: 50890\n",
            "Iteration: 600. Loss: 0.19823502004146576. Accuracy: 95\n",
            "Parameters: 50890\n",
            "Iteration: 1200. Loss: 0.1227787509560585. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 1800. Loss: 0.16046862304210663. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 2400. Loss: 0.09352443367242813. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 3000. Loss: 0.02999391034245491. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 3600. Loss: 0.04375356063246727. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 4200. Loss: 0.04025638476014137. Accuracy: 97\n",
            "Parameters: 50890\n",
            "Iteration: 4800. Loss: 0.05508409067988396. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 5400. Loss: 0.14829429984092712. Accuracy: 96\n",
            "Parameters: 50890\n",
            "Iteration: 6000. Loss: 0.049088068306446075. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp-6EM_nuOKS",
        "colab_type": "text"
      },
      "source": [
        "**MODEL_2_singlehiddenlayer_ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc75Y_ocfN5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "9de2f582-f2e7-4a15-863a-94a965ed1e73"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "\n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR\n",
        "        out = self.fc1(x)\n",
        "\n",
        "        # Non-linearity  # NON-LINEAR\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim = 64\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 600 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 600. Loss: 0.10354761779308319. Accuracy: 95\n",
            "Iteration: 1200. Loss: 0.14353252947330475. Accuracy: 96\n",
            "Iteration: 1800. Loss: 0.11414793878793716. Accuracy: 96\n",
            "Iteration: 2400. Loss: 0.05229035019874573. Accuracy: 96\n",
            "Iteration: 3000. Loss: 0.08955443650484085. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQteNAC8zcPp",
        "colab_type": "text"
      },
      "source": [
        "**Model_3_2hiddenlayers_ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R10KT83IfN8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "a02cb4fc-dfa6-4de9-e2b1-172f065b18d0"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1,hidden_dim12, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1 (784 --> 100)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1) \n",
        "        # Non-linearity 1\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Linear Function 2: (100 --> 100)\n",
        "        self.fc2 = nn.Linear(hidden_dim1,hidden_dim2)\n",
        "        # Non-Linearity 2\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity  # NON-LINEAR 1\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        #Linear Function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non - linearity 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2= 32\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim,hidden_dim1,hidden_dim2, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "learning_rate = 0.2\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = float(100 * correct / total)\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.5825788378715515. Accuracy: 86.0\n",
            "Iteration: 1000. Loss: 0.2996422052383423. Accuracy: 92.0\n",
            "Iteration: 1500. Loss: 0.2911902964115143. Accuracy: 93.0\n",
            "Iteration: 2000. Loss: 0.13938476145267487. Accuracy: 93.0\n",
            "Iteration: 2500. Loss: 0.2750631868839264. Accuracy: 93.0\n",
            "Iteration: 3000. Loss: 0.22518295049667358. Accuracy: 94.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S783tm7tnkdc",
        "colab_type": "text"
      },
      "source": [
        "**Model_4_2hiddenlayers_Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKXp3y-6NDAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "1b69bedf-e817-499b-cc95-f11c3925cccf"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1,hidden_dim12, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1 (784 --> 100)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1) \n",
        "        # Non-linearity 1\n",
        "        self.sigmoid1 = nn.Sigmoid()\n",
        "\n",
        "        # Linear Function 2: (100 --> 100)\n",
        "        self.fc2 = nn.Linear(hidden_dim1,hidden_dim2)\n",
        "        # Non-Linearity 2\n",
        "        self.sigmoid2 = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function  # LINEAR 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity  # NON-LINEAR 1\n",
        "        out = self.sigmoid1(out)\n",
        "\n",
        "        #Linear Function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non - linearity 2\n",
        "        out = self.sigmoid2(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2= 32\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim,hidden_dim1,hidden_dim2, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = float(100 * correct / total)\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: 52650\n",
            "Iteration: 500. Loss: 2.2086191177368164. Accuracy: 50.0\n",
            "Parameters: 52650\n",
            "Iteration: 1000. Loss: 1.2478301525115967. Accuracy: 63.0\n",
            "Parameters: 52650\n",
            "Iteration: 1500. Loss: 0.7558260560035706. Accuracy: 80.0\n",
            "Parameters: 52650\n",
            "Iteration: 2000. Loss: 0.4983283281326294. Accuracy: 85.0\n",
            "Parameters: 52650\n",
            "Iteration: 2500. Loss: 0.5372934341430664. Accuracy: 87.0\n",
            "Parameters: 52650\n",
            "Iteration: 3000. Loss: 0.40647831559181213. Accuracy: 89.0\n",
            "Parameters: 52650\n",
            "Iteration: 3500. Loss: 0.33674943447113037. Accuracy: 89.0\n",
            "Parameters: 52650\n",
            "Iteration: 4000. Loss: 0.3637329936027527. Accuracy: 90.0\n",
            "Parameters: 52650\n",
            "Iteration: 4500. Loss: 0.29772165417671204. Accuracy: 90.0\n",
            "Parameters: 52650\n",
            "Iteration: 5000. Loss: 0.3259194493293762. Accuracy: 91.0\n",
            "Parameters: 52650\n",
            "Iteration: 5500. Loss: 0.19395916163921356. Accuracy: 91.0\n",
            "Parameters: 52650\n",
            "Iteration: 6000. Loss: 0.31262481212615967. Accuracy: 92.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouqhag7NzT3d",
        "colab_type": "text"
      },
      "source": [
        "**Model_4_3hiddenlayers_ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxb-PNvJf2Pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e16814ea-1142-4cca-9266-94c956be6cdc"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1,hidden_dim2,hidden_dim3, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1) \n",
        "        # Non-linearity 1\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Linear function 2\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        # Non-linearity 2\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Linear function 3\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        # Non-linearity 3\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Linear function 4 (readout):\n",
        "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity 1\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc3(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        # Linear function 4 (readout)\n",
        "        out = self.fc4(out)\n",
        "        return out# Linear function  # LINEAR 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity  # NON-LINEAR 1\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        #Linear Function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non - linearity 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2= 32\n",
        "hidden_dim3= 16\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim1,hidden_dim2,hidden_dim3, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 600 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: 53018\n",
            "Iteration: 600. Loss: 0.40132859349250793. Accuracy: 90\n",
            "Parameters: 53018\n",
            "Iteration: 1200. Loss: 0.24722160398960114. Accuracy: 92\n",
            "Parameters: 53018\n",
            "Iteration: 1800. Loss: 0.06277034431695938. Accuracy: 95\n",
            "Parameters: 53018\n",
            "Iteration: 2400. Loss: 0.09136150032281876. Accuracy: 96\n",
            "Parameters: 53018\n",
            "Iteration: 3000. Loss: 0.14660100638866425. Accuracy: 96\n",
            "Parameters: 53018\n",
            "Iteration: 3600. Loss: 0.22932130098342896. Accuracy: 96\n",
            "Parameters: 53018\n",
            "Iteration: 4200. Loss: 0.07837072759866714. Accuracy: 96\n",
            "Parameters: 53018\n",
            "Iteration: 4800. Loss: 0.050865963101387024. Accuracy: 96\n",
            "Parameters: 53018\n",
            "Iteration: 5400. Loss: 0.07254394888877869. Accuracy: 96\n",
            "Parameters: 53018\n",
            "Iteration: 6000. Loss: 0.03610512986779213. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enzp4chSf2UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqa3X3WXtjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0s63M82n6nD",
        "colab_type": "text"
      },
      "source": [
        "**Model_5_3hiddenlayers_Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vknpPKdTXtgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "bc380832-aef6-45e5-dda9-a6572e501e4e"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1,hidden_dim2,hidden_dim3, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1) \n",
        "        # Non-linearity 1\n",
        "        self.sigmoid1 = nn.Sigmoid()\n",
        "\n",
        "        # Linear function 2\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        # Non-linearity 2\n",
        "        self.sigmoid2 = nn.Sigmoid()\n",
        "\n",
        "        # Linear function 3\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        # Non-linearity 3\n",
        "        self.sigmoid3 = nn.Sigmoid()\n",
        "\n",
        "        # Linear function 4 (readout):\n",
        "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity 1\n",
        "        out = self.sigmoid1(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.sigmoid2(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc3(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.sigmoid3(out)\n",
        "\n",
        "        # Linear function 4 (readout)\n",
        "        out = self.fc4(out)\n",
        "        return out# Linear function  # LINEAR 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity  # NON-LINEAR 1\n",
        "        out = self.sigmoid1(out)\n",
        "\n",
        "        #Linear Function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non - linearity 2\n",
        "        out = self.sigmoid2(out)\n",
        "\n",
        "        # Linear function (readout)  # LINEAR\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2= 32\n",
        "hidden_dim3= 16\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim1,hidden_dim2,hidden_dim3, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 600 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: 53018\n",
            "Iteration: 600. Loss: 2.3088741302490234. Accuracy: 11\n",
            "Parameters: 53018\n",
            "Iteration: 1200. Loss: 2.30619215965271. Accuracy: 11\n",
            "Parameters: 53018\n",
            "Iteration: 1800. Loss: 2.295761823654175. Accuracy: 11\n",
            "Parameters: 53018\n",
            "Iteration: 2400. Loss: 2.285662889480591. Accuracy: 11\n",
            "Parameters: 53018\n",
            "Iteration: 3000. Loss: 2.2355639934539795. Accuracy: 21\n",
            "Parameters: 53018\n",
            "Iteration: 3600. Loss: 1.4160528182983398. Accuracy: 46\n",
            "Parameters: 53018\n",
            "Iteration: 4200. Loss: 1.2083994150161743. Accuracy: 56\n",
            "Parameters: 53018\n",
            "Iteration: 4800. Loss: 1.04866361618042. Accuracy: 64\n",
            "Parameters: 53018\n",
            "Iteration: 5400. Loss: 0.8724411129951477. Accuracy: 78\n",
            "Parameters: 53018\n",
            "Iteration: 6000. Loss: 0.5630941987037659. Accuracy: 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhfv46fZoCbQ",
        "colab_type": "text"
      },
      "source": [
        "**Model_6_4hiddenlayers_Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojrZzlvPY5ru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e149eca9-9d22-48cc-8744-123d5bfa2de6"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1,hidden_dim2,hidden_dim3,hidden_dim4, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
        "        self.fc5 = nn.Linear(hidden_dim4, output_dim)\n",
        "        # Non-linearity 1\n",
        "        self.sigmoid1 = nn.Sigmoid()\n",
        "        self.sigmoid2 = nn.Sigmoid()\n",
        "        self.sigmoid3 = nn.Sigmoid()\n",
        "        self.sigmoid4 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function 1\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        # Non-linearity 1\n",
        "        out = self.sigmoid1(out)\n",
        "        out = self.sigmoid2(out)\n",
        "        out = self.sigmoid3(out)\n",
        "        out = self.sigmoid4(out)\n",
        "      \n",
        "        return out\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim1 = 128\n",
        "hidden_dim2 = 64\n",
        "hidden_dim3 = 32\n",
        "hidden_dim4 = 16\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim1,hidden_dim2,hidden_dim3,hidden_dim4, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 600 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: 111514\n",
            "Iteration: 600. Loss: 2.3025522232055664. Accuracy: 9\n",
            "Parameters: 111514\n",
            "Iteration: 1200. Loss: 2.302579879760742. Accuracy: 10\n",
            "Parameters: 111514\n",
            "Iteration: 1800. Loss: 2.302558422088623. Accuracy: 11\n",
            "Parameters: 111514\n",
            "Iteration: 2400. Loss: 2.302553415298462. Accuracy: 13\n",
            "Parameters: 111514\n",
            "Iteration: 3000. Loss: 2.3024749755859375. Accuracy: 13\n",
            "Parameters: 111514\n",
            "Iteration: 3600. Loss: 2.3025853633880615. Accuracy: 14\n",
            "Parameters: 111514\n",
            "Iteration: 4200. Loss: 2.302595615386963. Accuracy: 15\n",
            "Parameters: 111514\n",
            "Iteration: 4800. Loss: 2.302501678466797. Accuracy: 15\n",
            "Parameters: 111514\n",
            "Iteration: 5400. Loss: 2.302471399307251. Accuracy: 16\n",
            "Parameters: 111514\n",
            "Iteration: 6000. Loss: 2.3024606704711914. Accuracy: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0b4SMq7oMMo",
        "colab_type": "text"
      },
      "source": [
        "**Model_7_4hiddenlayers_ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvd6htugdRHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "3e3caa17-8fdb-41eb-82c4-1ac90b2a1438"
      },
      "source": [
        "## Step 3: Create Model Class\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1,hidden_dim2,hidden_dim3,hidden_dim4, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
        "        self.fc5 = nn.Linear(hidden_dim4, output_dim)\n",
        "        # Non-linearity 1\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function 1\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        # Non-linearity 1\n",
        "        out = self.relu1(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.relu4(out)\n",
        "      \n",
        "        return out#\n",
        "\n",
        "\n",
        "## Step 4: Instantiate Model Class\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim1 = 128\n",
        "hidden_dim2 = 64\n",
        "hidden_dim3 = 32\n",
        "hidden_dim4 = 16\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim1,hidden_dim2,hidden_dim3,hidden_dim4, output_dim)\n",
        "\n",
        "## Step 5: Instantiate Loss Class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "##STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.02,momentum=.9)\n",
        "\n",
        "## Step 7: Train Model\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 600 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            parameters=sum(p.numel() for p in model.parameters())\n",
        "            print(\"Parameters:\",parameters)\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: 111514\n",
            "Iteration: 600. Loss: 0.5022249817848206. Accuracy: 89\n",
            "Parameters: 111514\n",
            "Iteration: 1200. Loss: 0.17718960344791412. Accuracy: 90\n",
            "Parameters: 111514\n",
            "Iteration: 1800. Loss: 0.3479868769645691. Accuracy: 90\n",
            "Parameters: 111514\n",
            "Iteration: 2400. Loss: 0.3205270767211914. Accuracy: 91\n",
            "Parameters: 111514\n",
            "Iteration: 3000. Loss: 0.25322726368904114. Accuracy: 91\n",
            "Parameters: 111514\n",
            "Iteration: 3600. Loss: 0.22225545346736908. Accuracy: 91\n",
            "Parameters: 111514\n",
            "Iteration: 4200. Loss: 0.6856802105903625. Accuracy: 91\n",
            "Parameters: 111514\n",
            "Iteration: 4800. Loss: 0.34979239106178284. Accuracy: 91\n",
            "Parameters: 111514\n",
            "Iteration: 5400. Loss: 0.35401150584220886. Accuracy: 91\n",
            "Parameters: 111514\n",
            "Iteration: 6000. Loss: 0.2134648561477661. Accuracy: 90\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}